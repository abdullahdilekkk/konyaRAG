import os
import requests
from config.settings import LLM_MODEL_NAME

# DÄ°KKAT Ã–ÄRETÄ°CÄ° NOT:
# RAG sistemlerinde "Generation (Ãœretme)" adÄ±mÄ±, arabulucu (sekreter) gibidir.
# Ã–nceki dosyada bulduÄŸu metinleri ve kullanÄ±cÄ±nÄ±n sorusunu birleÅŸtirerek
# "LÃ¼tfen bu metinlere bakarak bu soruya cevap ver" diyen bÃ¼yÃ¼k bir metin (Prompt) hazÄ±rlar.
# ArdÄ±ndan bunu ChatGPT, LLaMA veya Claude gibi bir modele yollar.

def secilen_metinler_ile_cevap_uret(kullanici_sorusu: str, bulunan_metinler: list):
    """
    Bu fonksiyon:
    1. KullanÄ±cÄ±nÄ±n sorusunu alÄ±r.
    2. Milvus'tan (retrieval.py) dÃ¶nen alakalÄ± metinleri alÄ±r.
    3. Hepsini birleÅŸtirip "Prompt" adÄ± verilen bir emir kÃ¢ÄŸÄ±dÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
    4. Bu emri BÃ¼yÃ¼k Dil Modeline (LLM - yapay zekaya) yollar.
    5. Yapay zekanÄ±n "insan gibi" verdiÄŸi TÃ¼rkÃ§e cevabÄ± ekrana dÃ¶ndÃ¼rÃ¼r.
    """
    
    # EÄŸer Milvus hiÃ§bir metin bulamadÄ±ysa boÅŸuna yapay zekayÄ± yormayalÄ±m.
    if not bulunan_metinler:
        return "ÃœzgÃ¼nÃ¼m, soruna dair veritabanÄ±mda hiÃ§bir bilgi bulamadÄ±m."
        
    # =========================================================
    # Milvus'tan dÃ¶nen liste (bulunan_metinler) iÃ§indeki tÃ¼m metinleri aralarÄ±na boÅŸluk koyarak
    # tek bir devasa "bilgi metni" haline getiriyoruz.
    bilgi_yakiti = "\n".join(bulunan_metinler)
    
    # SENÄ°OR DEBUG (HATA AYIKLAMA) NOTU:
    toplam_kelime = len(bilgi_yakiti.split())
    toplam_harf = len(bilgi_yakiti)
    
    # Ekrana tÃ¼m PDF sayfalarÄ±nÄ± basÄ±p terminali Ã§Ã¶pe Ã§evirmek yerine, LLM'in ne kadar veri okuduÄŸunu
    # istatistiksel ve profesyonel bir ÅŸekilde (Enterprise tarzÄ±) Ã¶zetliyoruz.
    print(f"\n--- ğŸ” MÄ°LVUS BAÄLAM Ã–ZETÄ° ---")
    print(f"LLM'e Toplam: {toplam_kelime} kelimelik (yaklaÅŸÄ±k {toplam_harf} karakter) bilgi gÃ¶nderildi.")
    print("----------------------------------------------------------\n")
    
    # Modelin gÃ¶revi yanlÄ±ÅŸ anlamamasÄ± iÃ§in ona sert bir kural (Prompt) yazÄ±yoruz:
    emir_kagidi = f"""
    Sen, alanÄ±nda uzman akÄ±llÄ± bir asistansÄ±n. Ã‡evrendeki bilgilere dayanarak sorularÄ± cevapla.
    Asla bilmediÄŸin veya sana verilmeyen bir bilgiyi uydurma. Verilen bilgilerde cevabÄ± bulamazsan sadece 'Bilmiyorum' de.
    DÄ°KKAT: Cevap verirken ASLA 'Verilen metne gÃ¶re', 'Belgede belirtildiÄŸi gibi' veya 'Gelen bilgilere bakÄ±lÄ±rsa' gibi kalÄ±plar kullanma! 
    Sanki o bilgiyi sen zaten yÄ±llardÄ±r biliyormuÅŸsun gibi son derece doÄŸal, doÄŸrudan ve akÄ±cÄ± bir insan cÃ¼mlesi kur.
    
    --- SENÄ°N HAFIZANDAKÄ° BÄ°LGÄ°LER ---
    {bilgi_yakiti}
    ---
    
    Soru: {kullanici_sorusu}
    
    Cevap:
    """
    
    # 1. KAPI (Endpoint): Ä°stek atacaÄŸÄ±mÄ±z URL adresi.
    OLLAMA_URL = "http://localhost:11434/api/generate"
    
    payload = {
        "model": LLM_MODEL_NAME, # Hangi modeli kullanacaÄŸÄ±z? (Ã–rn: "llama3", "mistral" veya settings'teki model)
        "prompt": emir_kagidi,
        "stream": True # HIZLANDIRMA: CevabÄ± tek seferde beklemiyoruz, ChatGPT gibi akÄ±tarak alÄ±yoruz (True)
    }
    
    # Ollama'ya kendi bilgisayarÄ±mÄ±zdaki (localhost) sistemden JSON formatÄ±nda istek atÄ±yoruz:
    gelen_cevap = requests.post(OLLAMA_URL, json=payload, stream=True)
    
    # OLLAMA BÄ°R HATA DÃ–NDÃœRDÃœYSE (Model yok, silinmiÅŸ veya port yanlÄ±ÅŸsa)
    if gelen_cevap.status_code != 200:
        yield f"OLLAMA HATASI! (Kod: {gelen_cevap.status_code}) -> {gelen_cevap.text}"
        return
    
    # HIZLI VE AKICI YANIT (Streaming Parsing):
    import json
    for satir in gelen_cevap.iter_lines():
        if satir:
            veri = json.loads(satir)
            yield veri.get("response", "")


if __name__ == "__main__":
    # Ãœst klasÃ¶rdeki 'retrieval' dosyasÄ±ndan arama fonksiyonumuzu iÃ§eriye dahil ediyoruz (Ä°thal ediyoruz)
    from retrieval import soruyu_milvusta_ara 
    
    print("=====================================================")
    print(" KONYA RAG SÄ°STEMÄ°NE HOÅGELDÄ°NÄ°Z (Test Modu)")
    print(" Ã‡Ä±kmak iÃ§in 'q' tuÅŸuna basÄ±p Enter'a basabilirsiniz.")
    print("=====================================================\n")
    
    while True:
        # 1. Sisteme sormak istediÄŸimiz soruyu artÄ±k kodun iÃ§ine yazmÄ±yoruz,
        # Klavyeden (Terminalden) dinamik olarak o an ne sormak istiyorsak onu alÄ±yoruz:
        kralin_sorusu = input("LÃ¼tfen sorunuzu girin: ")
        
        # EÄŸer Ã§Ä±kmak istersek q yazÄ±p Ã§Ä±karÄ±z
        if kralin_sorusu.lower() == 'q':
            print("Sistemden Ã§Ä±kÄ±lÄ±yor. GÃ¶rÃ¼ÅŸmek Ã¼zere!")
            break
            
        print("\nâ³ Milvus VeritabanÄ±nda (HafÄ±zada) eÅŸleÅŸen parÃ§alar aranÄ±yor...")
        
        # 2. HafÄ±zadaki (Milvus'taki) ilgili PDF parÃ§acÄ±klarÄ±nÄ± arayÄ±p buluyoruz
        bulunan_parcalar = soruyu_milvusta_ara(kralin_sorusu, kac_cevap_getirsin=9)
        
        # 3. Bulunan bu parÃ§alarÄ± ve kullanÄ±cÄ±nÄ±n girdiÄŸi soruyu Yapay Zekaya gÃ¶nderiyoruz
        if bulunan_parcalar:
             print(f"Milvus'tan {len(bulunan_parcalar)} adet metin parÃ§asÄ± bulundu.")
             print("Åimdi Yapay Zeka (Ollama) cÃ¼mleyi toparlÄ±yor...\n")
             
             nihai_cevap = secilen_metinler_ile_cevap_uret(kullanici_sorusu=kralin_sorusu, bulunan_metinler=bulunan_parcalar)
             
             print("--- OLLAMA'NIN CEVABI ---")
             print(nihai_cevap)
             print("------------------------------------------------\n")
        else:
             print("Milvus'ta hiÃ§bir parÃ§a bulunamadÄ±! LÃ¼tfen Ã¶nce veritabanÄ±nÄ±n dolu olduÄŸundan emin ol.\n")