import os
import requests
from config.settings import LLM_MODEL_NAME

# DÄ°KKAT Ã–ÄRETÄ°CÄ° NOT:
# RAG sistemlerinde "Generation (Ãœretme)" adÄ±mÄ±, arabulucu (sekreter) gibidir.
# Ã–nceki dosyada bulduÄŸu metinleri ve kullanÄ±cÄ±nÄ±n sorusunu birleÅŸtirerek
# "LÃ¼tfen bu metinlere bakarak bu soruya cevap ver" diyen bÃ¼yÃ¼k bir metin (Prompt) hazÄ±rlar.
# ArdÄ±ndan bunu ChatGPT, LLaMA veya Claude gibi bir modele yollar.

def secilen_metinler_ile_cevap_uret(kullanici_sorusu: str, bulunan_metinler: list):
    """
    Bu fonksiyon:
    1. KullanÄ±cÄ±nÄ±n sorusunu alÄ±r.
    2. Milvus'tan (retrieval.py) dÃ¶nen alakalÄ± metinleri alÄ±r.
    3. Hepsini birleÅŸtirip "Prompt" adÄ± verilen bir emir kÃ¢ÄŸÄ±dÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
    4. Bu emri BÃ¼yÃ¼k Dil Modeline (LLM - yapay zekaya) yollar.
    5. Yapay zekanÄ±n "insan gibi" verdiÄŸi TÃ¼rkÃ§e cevabÄ± ekrana dÃ¶ndÃ¼rÃ¼r.
    """
    
    # EÄŸer Milvus hiÃ§bir metin bulamadÄ±ysa boÅŸuna yapay zekayÄ± yormayalÄ±m.
    if not bulunan_metinler:
        return "ÃœzgÃ¼nÃ¼m, soruna dair veritabanÄ±mda hiÃ§bir bilgi bulamadÄ±m."
        
    # =========================================================
    # Milvus'tan dÃ¶nen liste (bulunan_metinler) iÃ§indeki tÃ¼m metinleri aralarÄ±na boÅŸluk koyarak
    # tek bir devasa "bilgi metni" haline getiriyoruz.
    bilgi_yakiti = "\n".join(bulunan_metinler)
    
    # SENÄ°OR DEBUG (HATA AYIKLAMA) NOTU:
    # Yapay zekaya gitmeden Ã¶nce, veritabanÄ±ndan cÄ±mbÄ±zladÄ±ÄŸÄ±mÄ±z o 3 metin parÃ§asÄ± acaba neymiÅŸ?
    # GerÃ§ekten iÃ§inde etliekmek geÃ§iyor muymuÅŸ? Kendimiz okuyalÄ±m diye ekrana basÄ±yoruz:
    print("\n--- ğŸ” MÄ°LVUS'TAN GELEN HAM BÄ°LGÄ° METNÄ° (LLM'in OkuduÄŸu) ---")
    print(bilgi_yakiti)
    print("----------------------------------------------------------\n")
    
    # Modelin gÃ¶revi yanlÄ±ÅŸ anlamamasÄ± iÃ§in ona sert bir kural (Prompt) yazÄ±yoruz:
    emir_kagidi = f"""
    Sen, sadece sana verilen 'BÄ°LGÄ°' metnine dayanarak sorularÄ± cevaplayan akÄ±llÄ± bir asistansÄ±n.
    Asla BÄ°LGÄ° kÄ±smÄ±nda geÃ§meyen bir yeteneÄŸi veya yorumu uydurma. Bilgi yoksa "Bilmiyorum" de.
    
    --- BÄ°LGÄ° BAÅLANGICI ---
    {bilgi_yakiti}
    --- BÄ°LGÄ° BÄ°TÄ°ÅÄ° ---
    
    Soru: {kullanici_sorusu}
    
    Cevap:
    """
    
    # 1. KAPI (Endpoint): Ä°stek atacaÄŸÄ±mÄ±z URL adresi.
    OLLAMA_URL = "http://localhost:11434/api/generate"
    
    payload = {
        "model": LLM_MODEL_NAME, # Hangi modeli kullanacaÄŸÄ±z? (Ã–rn: "llama3", "mistral" veya settings'teki model)
        "prompt": emir_kagidi,
        "stream": False # CevabÄ± kelime kelime yavaÅŸÃ§a deÄŸil, tamamen bitince tek seferde istiyoruz (False)
    }
    
    # Ollama'ya kendi bilgisayarÄ±mÄ±zdaki (localhost) sistemden JSON formatÄ±nda istek atÄ±yoruz:
    gelen_cevap = requests.post(OLLAMA_URL, json=payload)
    
    # OLLAMA BÄ°R HATA DÃ–NDÃœRDÃœYSE (Model yok, silinmiÅŸ veya port yanlÄ±ÅŸsa)
    if gelen_cevap.status_code != 200:
        return f"OLLAMA HATASI! (Kod: {gelen_cevap.status_code}) -> {gelen_cevap.text}"
    
    # 4. CEVABI AÃ‡ (Response Parsing):
    # KapÄ±dan dÃ¶nen kargoyu Ã¶nce bilgisayarÄ±n anladÄ±ÄŸÄ± dilden JSON sÃ¶zlÃ¼ÄŸÃ¼ne Ã§eviriyoruz
    sonuc_json = gelen_cevap.json()
    
    # O sÃ¶zlÃ¼ÄŸÃ¼n iÃ§inden asÄ±l TÃ¼rkÃ§e cÃ¼mlenin yazdÄ±ÄŸÄ± "response" etiketini cÄ±mbÄ±zla Ã§ekiyoruz
    olusan_cevap = sonuc_json.get("response", "HATA: Ollama'dan baÅŸarÄ±lÄ± dÃ¶ndÃ¼ ama iÃ§i boÅŸ.")
    
    return olusan_cevap.strip()


if __name__ == "__main__":
    # Ãœst klasÃ¶rdeki 'retrieval' dosyasÄ±ndan arama fonksiyonumuzu iÃ§eriye dahil ediyoruz (Ä°thal ediyoruz)
    from retrieval import soruyu_milvusta_ara 
    
    print("=====================================================")
    print(" KONYA RAG SÄ°STEMÄ°NE HOÅGELDÄ°NÄ°Z (Test Modu)")
    print(" Ã‡Ä±kmak iÃ§in 'q' tuÅŸuna basÄ±p Enter'a basabilirsiniz.")
    print("=====================================================\n")
    
    while True:
        # 1. Sisteme sormak istediÄŸimiz soruyu artÄ±k kodun iÃ§ine yazmÄ±yoruz,
        # Klavyeden (Terminalden) dinamik olarak o an ne sormak istiyorsak onu alÄ±yoruz:
        kralin_sorusu = input("LÃ¼tfen sorunuzu girin: ")
        
        # EÄŸer Ã§Ä±kmak istersek q yazÄ±p Ã§Ä±karÄ±z
        if kralin_sorusu.lower() == 'q':
            print("Sistemden Ã§Ä±kÄ±lÄ±yor. GÃ¶rÃ¼ÅŸmek Ã¼zere!")
            break
            
        print("\nâ³ Milvus VeritabanÄ±nda (HafÄ±zada) eÅŸleÅŸen parÃ§alar aranÄ±yor...")
        
        # 2. HafÄ±zadaki (Milvus'taki) ilgili PDF parÃ§acÄ±klarÄ±nÄ± arayÄ±p buluyoruz
        bulunan_parcalar = soruyu_milvusta_ara(kralin_sorusu, kac_cevap_getirsin=3)
        
        # 3. Bulunan bu parÃ§alarÄ± ve kullanÄ±cÄ±nÄ±n girdiÄŸi soruyu Yapay Zekaya gÃ¶nderiyoruz
        if bulunan_parcalar:
             print(f"Milvus'tan {len(bulunan_parcalar)} adet metin parÃ§asÄ± bulundu.")
             print("Åimdi Yapay Zeka (Ollama) cÃ¼mleyi toparlÄ±yor...\n")
             
             nihai_cevap = secilen_metinler_ile_cevap_uret(kullanici_sorusu=kralin_sorusu, bulunan_metinler=bulunan_parcalar)
             
             print("--- OLLAMA'NIN CEVABI ---")
             print(nihai_cevap)
             print("------------------------------------------------\n")
        else:
             print("Milvus'ta hiÃ§bir parÃ§a bulunamadÄ±! LÃ¼tfen Ã¶nce veritabanÄ±nÄ±n dolu olduÄŸundan emin ol.\n")